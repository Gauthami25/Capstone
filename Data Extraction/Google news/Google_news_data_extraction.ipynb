{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpqQoKWjscQf",
        "outputId": "cd6e4a1b-3186-4976-8ad1-3550c9dd81fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygooglenews\n",
            "  Downloading pygooglenews-0.1.2-py3-none-any.whl (10 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.2-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dateparser<0.8.0,>=0.7.6\n",
            "  Downloading dateparser-0.7.6-py2.py3-none-any.whl (362 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.0/362.0 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedparser<6.0.0,>=5.2.1\n",
            "  Downloading feedparser-5.2.1.zip (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.8/dist-packages (from pygooglenews) (2.25.1)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (2.8.2)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.8/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (2022.6.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from dateparser<0.8.0,>=0.7.6->pygooglenews) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->pygooglenews) (4.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->dateparser<0.8.0,>=0.7.6->pygooglenews) (1.15.0)\n",
            "Building wheels for collected packages: feedparser\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-py3-none-any.whl size=44951 sha256=95628f3d115da105a2aee0cb505e3b4d2f11260d425ac001032a5f5fc30bbd6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/38/07/59c89c3d334e7f1f743898af6d21c15ecb3588ad04af7ddee0\n",
            "Successfully built feedparser\n",
            "Installing collected packages: feedparser, soupsieve, dateparser, beautifulsoup4, pygooglenews\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.2 dateparser-0.7.6 feedparser-5.2.1 pygooglenews-0.1.2 soupsieve-2.3.2.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install pygooglenews"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pygooglenews import GoogleNews\n"
      ],
      "metadata": {
        "id": "zch9k8czshNf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gn = GoogleNews()\n",
        "search = gn.search('climate')\n",
        "\n",
        "for item in search['entries']:\n",
        "  print(item['title'])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ_zlokTsvf_",
        "outputId": "4b56208b-592a-4e59-fcc8-d1d8f6307712"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Climate change and a population boom could dry up the Great Salt Lake in 5 years - NPR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfAoPQHcyzSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "import pandas as pd\n",
        "def get_titles(keyword):\n",
        "  news= []\n",
        "  gn=GoogleNews(lang='en')\n",
        "  search = gn.search(keyword)\n",
        "  # print(search)\n",
        "\n",
        "\n",
        "  # pprint.pprint(search)\n",
        "\n",
        "  articles = search['entries']\n",
        "  \n",
        "  for i in articles:\n",
        "    # pprint.pprint(i)\n",
        "    # print(i.keys())\n",
        "    # print(i['source']['title'])\n",
        "    # print(i['title'].split('-')[0].strip())\n",
        "    # pprint.pprint(i['feed']['source']['title'])\n",
        "    article= {\"Publication\": i['source']['title'], \"Title\": i['title'].split('-')[0].strip(), \"Date\": i['published']}\n",
        "    news.append(article)\n",
        "\n",
        "  news_df = pd.DataFrame(news)\n",
        "  news_df.to_csv(\"Google_news_sustainability.csv\", index = False)\n",
        "    \n",
        "  #  news.append(article)\n",
        "  # return news\n",
        "\n",
        "data = get_titles(\"sustainability\")\n",
        "# data"
      ],
      "metadata": {
        "id": "Ibws9MR1s99r"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYb-5nSCY_LB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}